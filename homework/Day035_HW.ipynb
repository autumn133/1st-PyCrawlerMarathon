{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加速：多線程爬蟲\n",
    "\n",
    "\n",
    "\n",
    "* 了解知乎 API 使用方式與回傳內容\n",
    "* 撰寫程式存取 API 且添加標頭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* 找一個之前實作過的爬蟲改用多線程改寫，比較前後時間的差異。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your Code\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import _thread\n",
    "url = \"https://www.ptt.cc/bbs/hotboards.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.05597686767578\n"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "lists = soup.find_all('div',{'class':'b-ent'})\n",
    "startTime = time.time()\n",
    "for list in lists:\n",
    "    linkUrl = 'https://www.ptt.cc'.format(list.a['href'])\n",
    "    res1 = requests.get(linkUrl)\n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02800154685974121\n"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "lists = soup.find_all('div',{'class':'b-ent'})\n",
    "startTime = time.time()\n",
    "for list in lists:\n",
    "    linkUrl = 'https://www.ptt.cc'.format(list.a['href'])\n",
    "    _thread.start_new_thread( requests.get, (linkUrl, ) )\n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import _thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 這裡先建立一個函數，其功能是連到外部連結，並爬取新聞內容。\n",
    "#\n",
    "def getNewsDetailContent(link_url):\n",
    "    resp = requests.get(link_url)\n",
    "    resp.encoding = 'utf-8'\n",
    "    #print(resp.text)\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    news_content = soup.find(\"div\", attrs={'class':'story'}).find_all(\"p\")\n",
    "#     for p in news_content:\n",
    "#         \"\"\"\n",
    "#         .string屬性說明：\n",
    "#         (1) 若當前tag節點底下沒有其他tag子節點，會直接抓取內容(返回\"NavigableString\")\n",
    "#         (2) 若當前tag節點底下只有唯一的一個tag子節點，也會直接抓取tag子節點的內容(返回\"NavigableString\")\n",
    "#         (3) 但若當前tag節點底下還有很多個tag子節點，.string就無法判斷，(返回\"None\")\n",
    "#         \"\"\"\n",
    "#         if ((p.string) is not None):\n",
    "#             print(p.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "網頁更新中...\n",
      "到達頁面底端\n"
     ]
    }
   ],
   "source": [
    "ETtoday_url = \"https://www.ettoday.net/news/focus/%E8%B2%A1%E7%B6%93/\"  #財經新聞\n",
    "\n",
    "browser = webdriver.Chrome(executable_path='chromedriver')\n",
    "# browser = webdriver.Chrome(executable_path='./Data/chromedriver')\n",
    "browser.get(ETtoday_url)  # 打開瀏覽器並連到東森新聞雲網頁\n",
    "\n",
    "#\n",
    "# 以下是用Selenium模擬下拉網頁動作，讓網頁更新\n",
    "#\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "while True:\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    current_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "    \n",
    "    if current_height == last_height:\n",
    "        print('到達頁面底端')\n",
    "        break\n",
    "    else:\n",
    "        print('網頁更新中...')\n",
    "        last_height = current_height\n",
    "        continue\n",
    "        \n",
    "# 爬取網頁內容，解析後萃取新聞摘要\n",
    "html = browser.page_source\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "all_news = soup.find(\"div\", attrs={'class':'block block_1 infinite_scroll'})\n",
    "news_block = all_news.find_all('div', attrs={'class':'piece clearfix'})\n",
    "\n",
    "# 關閉瀏覽器\n",
    "browser.quit();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.145347118377686\n"
     ]
    }
   ],
   "source": [
    "# 單線程\n",
    "startTime = time.time()\n",
    "for i, news_item in enumerate(news_block):\n",
    "#     print(\"----------------------------------------------------------------------\")\n",
    "    news_body = news_item.find('h3')\n",
    "#     print(\"\\n[%d] %s\\n\" % (i, news_body.a.string))\n",
    "    \n",
    "    # \n",
    "    # 連到外部連結，擷取詳細新聞內容\n",
    "    #\n",
    "    externalLink = \"https://www.ettoday.net\" + news_body.a[\"href\"]\n",
    "    getNewsDetailContent(externalLink)\n",
    "    finishTime = time.time()\n",
    "print(finishTime - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013321638107299805\n"
     ]
    }
   ],
   "source": [
    "# 多線程\n",
    "startTime = time.time()\n",
    "for i, news_item in enumerate(news_block):\n",
    "#     print(\"----------------------------------------------------------------------\")\n",
    "    news_body = news_item.find('h3')\n",
    "#     print(\"\\n[%d] %s\\n\" % (i, news_body.a.string))\n",
    "    \n",
    "    # \n",
    "    # 連到外部連結，擷取詳細新聞內容\n",
    "    #\n",
    "    externalLink = \"https://www.ettoday.net\" + news_body.a[\"href\"]\n",
    "    _thread.start_new_thread(getNewsDetailContent, (externalLink, ))\n",
    "    finishTime = time.time()\n",
    "print(finishTime - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [問卦] 可以痛扁還去日本玩的人嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1582875823.A.33D.html\n",
      "Parse Re: [新聞] 洪慈庸將任立院顧問 游錫堃：機要缺不算 - https://www.ptt.cc/bbs/Gossiping/M.1582875848.A.602.html\n",
      "Parse [問卦] 德義日韓算是新一代舔共軸心國嗎? - https://www.ptt.cc/bbs/Gossiping/M.1582875901.A.2AE.html\n",
      "Parse Re: [問卦] 對面套房內有基地台怎麼辦？ - https://www.ptt.cc/bbs/Gossiping/M.1582875992.A.3A3.html\n",
      "Parse [新聞] 當年和平醫院逾一成陪病者染SARS 名醫籲 - https://www.ptt.cc/bbs/Gossiping/M.1582875995.A.5F5.html\n",
      "Parse [問卦] 求神拜佛有用的話 是不是醫院都可以關了 - https://www.ptt.cc/bbs/Gossiping/M.1582876006.A.96A.html\n",
      "Parse [問卦] 只有我會希望公司有人生病嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1582876025.A.9EB.html\n",
      "Parse [問卦] 車禍幫忙抬車,如果手滑,車子掉下壓死人? - https://www.ptt.cc/bbs/Gossiping/M.1582876042.A.759.html\n",
      "Parse [問卦] 去日本前要儲值台灣價值嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1582876115.A.045.html\n",
      "Parse [問卦] 顧門口的員工薪水多少比較適合??? - https://www.ptt.cc/bbs/Gossiping/M.1582876117.A.D94.html\n",
      "Parse [問卦] 開蟑聖母現在在想什麼？ - https://www.ptt.cc/bbs/Gossiping/M.1582876145.A.0CA.html\n",
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876192.A.D3D.html\n",
      "Parse [問卦] 以後你小孩如果是同性戀 - https://www.ptt.cc/bbs/Gossiping/M.1582876282.A.6B2.html\n",
      "Parse [問卦] 現在洗保全冤案484想掩飾什麼 - https://www.ptt.cc/bbs/Gossiping/M.1582876321.A.AF3.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876342.A.98F.html\n",
      "Parse [問卦] 各國當局是不是發現隱匿疫情比較划算？ - https://www.ptt.cc/bbs/Gossiping/M.1582876442.A.0E7.html\n",
      "Parse [問卦] Corona？ - https://www.ptt.cc/bbs/Gossiping/M.1582876470.A.D38.html\n",
      "Parse Re: [新聞] 武漢肺炎》50多歲婦「無旅遊史」確診 指 - https://www.ptt.cc/bbs/Gossiping/M.1582876476.A.4D5.html\n",
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876504.A.D16.html\n",
      "Parse [問卦] 喜歡在字結尾加表情符號都4哪種人?☺ - https://www.ptt.cc/bbs/Gossiping/M.1582876508.A.F74.html\n",
      "Reach the last article\n",
      "共用時： 7.595657587051392\n"
     ]
    }
   ],
   "source": [
    "#單線程爬蟲\n",
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies={'over18': '1'})\n",
    "    \n",
    "    # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "        return\n",
    "    \n",
    "    # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # 取得文章內容主體\n",
    "    main_content = soup.find(id='main-content')\n",
    "    \n",
    "    # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "    metas = main_content.select('div.article-metaline') #list\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    if metas:\n",
    "        if metas[0].select('span.article-meta-value')[0]:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string\n",
    "        if metas[1].select('span.article-meta-value')[0]:\n",
    "            title = metas[1].select('span.article-meta-value')[0].string\n",
    "        if metas[2].select('span.article-meta-value')[0]:\n",
    "            date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "        # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "        #\n",
    "        # .extract() 方法可以參考官方文件\n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "        for m in metas:\n",
    "            m.extract()\n",
    "        for m in main_content.select('div.article-metaline-right'):\n",
    "            m.extract()\n",
    "    \n",
    "    # 取得留言區主體\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for p in pushes:\n",
    "        p.extract()\n",
    "    \n",
    "    # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "    # 透過 regular expression 取得 IP\n",
    "    # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except Exception as e:\n",
    "        ip = ''\n",
    "    \n",
    "    # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    #\n",
    "    # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "    #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "    filtered = []\n",
    "    for v in main_content.stripped_strings:\n",
    "        # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "        if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "            filtered.append(v)\n",
    "\n",
    "    # 定義一些特殊符號與全形符號的過濾器\n",
    "    expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "    \n",
    "    # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "    filtered = [i for i in filtered if i]\n",
    "    content = ' '.join(filtered)\n",
    "    \n",
    "    # 處理留言區\n",
    "    # p 計算推文數量\n",
    "    # b 計算噓文數量\n",
    "    # n 計算箭頭數量\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        # 假如留言段落沒有 push-tag 就跳過\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        \n",
    "        # 過濾額外空白與換行符號\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 整理打包留言的資訊, 並統計推噓文數量\n",
    "        messages.append({\n",
    "            'push_tag': push_tag,\n",
    "            'push_userid': push_userid,\n",
    "            'push_content': push_content,\n",
    "            'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    \n",
    "    # 統計推噓文\n",
    "    # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "    # all 為總共留言數量 \n",
    "    message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "    \n",
    "    # 整理文章資訊\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'article_author': author,\n",
    "        'article_title': title,\n",
    "        'article_date': date,\n",
    "        'article_content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    }\n",
    "    return data\n",
    "\n",
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 呼叫上面寫好的 function 來對文章進行爬蟲\n",
    "        if article_URL:\n",
    "            parse_data = crawl_article(article_URL) # 返回單一文章資訊的字典\n",
    "        \n",
    "        # 將爬完的資料儲存\n",
    "        all_data.append(parse_data)\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [問卦] 可以痛扁還去日本玩的人嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1582875823.A.33D.html\n",
      "Parse Re: [新聞] 洪慈庸將任立院顧問 游錫堃：機要缺不算 - https://www.ptt.cc/bbs/Gossiping/M.1582875848.A.602.html\n",
      "Parse [問卦] 德義日韓算是新一代舔共軸心國嗎? - https://www.ptt.cc/bbs/Gossiping/M.1582875901.A.2AE.html\n",
      "Parse Re: [問卦] 對面套房內有基地台怎麼辦？ - https://www.ptt.cc/bbs/Gossiping/M.1582875992.A.3A3.html\n",
      "Parse [新聞] 當年和平醫院逾一成陪病者染SARS 名醫籲 - https://www.ptt.cc/bbs/Gossiping/M.1582875995.A.5F5.html\n",
      "Parse [問卦] 求神拜佛有用的話 是不是醫院都可以關了 - https://www.ptt.cc/bbs/Gossiping/M.1582876006.A.96A.html\n",
      "Parse [問卦] 只有我會希望公司有人生病嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1582876025.A.9EB.html\n",
      "Parse [問卦] 車禍幫忙抬車,如果手滑,車子掉下壓死人? - https://www.ptt.cc/bbs/Gossiping/M.1582876042.A.759.html\n",
      "Parse [問卦] 去日本前要儲值台灣價值嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1582876115.A.045.html\n",
      "Parse [問卦] 顧門口的員工薪水多少比較適合??? - https://www.ptt.cc/bbs/Gossiping/M.1582876117.A.D94.html\n",
      "Parse [問卦] 開蟑聖母現在在想什麼？ - https://www.ptt.cc/bbs/Gossiping/M.1582876145.A.0CA.html\n",
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876192.A.D3D.html\n",
      "Parse [問卦] 以後你小孩如果是同性戀 - https://www.ptt.cc/bbs/Gossiping/M.1582876282.A.6B2.html\n",
      "Parse [問卦] 現在洗保全冤案484想掩飾什麼 - https://www.ptt.cc/bbs/Gossiping/M.1582876321.A.AF3.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876342.A.98F.html\n",
      "Parse [問卦] 各國當局是不是發現隱匿疫情比較划算？ - https://www.ptt.cc/bbs/Gossiping/M.1582876442.A.0E7.html\n",
      "Parse [問卦] Corona？ - https://www.ptt.cc/bbs/Gossiping/M.1582876470.A.D38.html\n",
      "Parse Re: [新聞] 武漢肺炎》50多歲婦「無旅遊史」確診 指 - https://www.ptt.cc/bbs/Gossiping/M.1582876476.A.4D5.html\n",
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876504.A.D16.html\n",
      "Parse [問卦] 喜歡在字結尾加表情符號都4哪種人?☺ - https://www.ptt.cc/bbs/Gossiping/M.1582876508.A.F74.html\n",
      "Reach the last article\n",
      "共用時： 1.0998802185058594\n"
     ]
    }
   ],
   "source": [
    "#多線程爬蟲\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies={'over18': '1'})\n",
    "    \n",
    "    # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "        return\n",
    "    \n",
    "    # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # 取得文章內容主體\n",
    "    main_content = soup.find(id='main-content')\n",
    "    \n",
    "    # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "    metas = main_content.select('div.article-metaline') #list\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    if metas:\n",
    "        if metas[0].select('span.article-meta-value')[0]:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string\n",
    "        if metas[1].select('span.article-meta-value')[0]:\n",
    "            title = metas[1].select('span.article-meta-value')[0].string\n",
    "        if metas[2].select('span.article-meta-value')[0]:\n",
    "            date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "        # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "        #\n",
    "        # .extract() 方法可以參考官方文件\n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "        for m in metas:\n",
    "            m.extract()\n",
    "        for m in main_content.select('div.article-metaline-right'):\n",
    "            m.extract()\n",
    "    \n",
    "    # 取得留言區主體\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for p in pushes:\n",
    "        p.extract()\n",
    "    \n",
    "    # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "    # 透過 regular expression 取得 IP\n",
    "    # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except Exception as e:\n",
    "        ip = ''\n",
    "    \n",
    "    # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    #\n",
    "    # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "    #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "    filtered = []\n",
    "    for v in main_content.stripped_strings:\n",
    "        # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "        if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "            filtered.append(v)\n",
    "\n",
    "    # 定義一些特殊符號與全形符號的過濾器\n",
    "    expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "    \n",
    "    # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "    filtered = [i for i in filtered if i]\n",
    "    content = ' '.join(filtered)\n",
    "    \n",
    "    # 處理留言區\n",
    "    # p 計算推文數量\n",
    "    # b 計算噓文數量\n",
    "    # n 計算箭頭數量\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        # 假如留言段落沒有 push-tag 就跳過\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        \n",
    "        # 過濾額外空白與換行符號\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 整理打包留言的資訊, 並統計推噓文數量\n",
    "        messages.append({\n",
    "            'push_tag': push_tag,\n",
    "            'push_userid': push_userid,\n",
    "            'push_content': push_content,\n",
    "            'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    \n",
    "    # 統計推噓文\n",
    "    # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "    # all 為總共留言數量 \n",
    "    message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "    \n",
    "    # 整理文章資訊\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'article_author': author,\n",
    "        'article_title': title,\n",
    "        'article_date': date,\n",
    "        'article_content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    }\n",
    "    return data\n",
    "\n",
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "all_url = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 把文章連結存在list\n",
    "        if article_URL:\n",
    "            all_url.append(article_URL)\n",
    "\n",
    "# 從這裡丟給子執行緒工作            \n",
    "# 建立 n 個子執行緒，分別去抓文章內容\n",
    "threads = []\n",
    "for i in range(len(all_url)):\n",
    "    threads.append(threading.Thread(target = crawl_article, args = (all_url[i],)))\n",
    "    threads[i].start()\n",
    "\n",
    "# 主執行緒繼續執行自己的工作\n",
    "# ...\n",
    "\n",
    "# 等待所有子執行緒結束\n",
    "for i in range(len(all_url)):\n",
    "    threads[i].join()\n",
    "\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get task: task_1\n",
      "\n",
      "Get task: task_2\n",
      "\n",
      "Get task: task_3\n",
      "\n",
      "Get task: task_4\n",
      "\n",
      "Get task: task_5\n",
      "\n",
      "Get task: task_6\n",
      "\n",
      "Get task: task_7\n",
      "\n",
      "Get task: task_8\n",
      "\n",
      "Get task: task_9\n",
      "\n",
      "Get task: task_10\n",
      "\n",
      "Finish task: task_1\n",
      "\n",
      "Finish task: task_2\n",
      "\n",
      "Finish task: task_3\n",
      "\n",
      "Finish task: task_4\n",
      "\n",
      "Finish task: task_5\n",
      "\n",
      "Finish task: task_6\n",
      "\n",
      "Finish task: task_7\n",
      "\n",
      "Finish task: task_8\n",
      "\n",
      "Finish task: task_9\n",
      "\n",
      "Finish task: task_10\n",
      "\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "#物件導向寫法\n",
    "class MyTask(threading.Thread):\n",
    "    def __init__(self, task_name):\n",
    "        super(MyTask, self).__init__()\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Get task: {}\\n\".format(self.task_name))\n",
    "        time.sleep(1)\n",
    "        print(\"Finish task: {}\\n\".format(self.task_name))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = [1,2,3,4,5,6,7,8,9,10]\n",
    "    tasks = []\n",
    "    for i in range(0, 10):\n",
    "        # 建立 task\n",
    "        tasks.append(MyTask(\"task_{}\".format(data[i])))\n",
    "    for t in tasks:\n",
    "        # 開始執行 task\n",
    "        t.start()\n",
    "\n",
    "    for t in tasks:\n",
    "        # 等待 task 執行完畢\n",
    "        # 完畢前會阻塞住主執行緒\n",
    "        t.join()\n",
    "    print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876536.A.B2E.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876557.A.92D.html\n",
      "Parse Re: [問卦] 開蟑聖母現在在想什麼？ - https://www.ptt.cc/bbs/Gossiping/M.1582876570.A.5F5.html\n",
      "Parse Re: [爆卦] 第三十三案、第三十四案 LIVE記者會 - https://www.ptt.cc/bbs/Gossiping/M.1582876582.A.45D.html\n",
      "Parse [問卦] 欸！究竟怎麼防中國網軍啊？ - https://www.ptt.cc/bbs/Gossiping/M.1582876639.A.08A.html\n",
      "Reach the last article\n",
      "共5個連結\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876536.A.B2E.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876557.A.92D.html\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876570.A.5F5.html\n",
      "\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876582.A.45D.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876639.A.08A.html\n",
      "\n",
      "共用時： 0.5022680759429932\n"
     ]
    }
   ],
   "source": [
    "#改寫\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.url = url\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        print(\"Get子執行緒: {}\\n\".format(self.url))\n",
    "\n",
    "        response = requests.get(self.url, cookies={'over18': '1'})\n",
    "\n",
    "        # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "        if response.status_code != 200:\n",
    "            print('Error - {} is not available to access'.format(self.url))\n",
    "            return\n",
    "\n",
    "        # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "        soup = BeautifulSoup(response.text)\n",
    "\n",
    "        # 取得文章內容主體\n",
    "        main_content = soup.find(id='main-content')\n",
    "\n",
    "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "        metas = main_content.select('div.article-metaline') #list\n",
    "        author = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        if metas:\n",
    "            if metas[0].select('span.article-meta-value')[0]:\n",
    "                author = metas[0].select('span.article-meta-value')[0].string\n",
    "            if metas[1].select('span.article-meta-value')[0]:\n",
    "                title = metas[1].select('span.article-meta-value')[0].string\n",
    "            if metas[2].select('span.article-meta-value')[0]:\n",
    "                date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "            # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "            #\n",
    "            # .extract() 方法可以參考官方文件\n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "            for m in metas:\n",
    "                m.extract()\n",
    "            for m in main_content.select('div.article-metaline-right'):\n",
    "                m.extract()\n",
    "\n",
    "        # 取得留言區主體\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for p in pushes:\n",
    "            p.extract()\n",
    "\n",
    "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "        # 透過 regular expression 取得 IP\n",
    "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except Exception as e:\n",
    "            ip = ''\n",
    "\n",
    "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "        #\n",
    "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "        filtered = []\n",
    "        for v in main_content.stripped_strings:\n",
    "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                filtered.append(v)\n",
    "\n",
    "        # 定義一些特殊符號與全形符號的過濾器\n",
    "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "        filtered = [i for i in filtered if i]\n",
    "        content = ' '.join(filtered)\n",
    "\n",
    "        # 處理留言區\n",
    "        # p 計算推文數量\n",
    "        # b 計算噓文數量\n",
    "        # n 計算箭頭數量\n",
    "        p, b, n = 0, 0, 0\n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            # 假如留言段落沒有 push-tag 就跳過\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue\n",
    "\n",
    "            # 過濾額外空白與換行符號\n",
    "            # push_tag 判斷是推文, 箭頭還是噓文\n",
    "            # push_userid 判斷留言的人是誰\n",
    "            # push_content 判斷留言內容\n",
    "            # push_ipdatetime 判斷留言日期時間\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            push_content = push.find('span', 'push-content').strings\n",
    "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "            # 整理打包留言的資訊, 並統計推噓文數量\n",
    "            messages.append({\n",
    "                'push_tag': push_tag,\n",
    "                'push_userid': push_userid,\n",
    "                'push_content': push_content,\n",
    "                'push_ipdatetime': push_ipdatetime})\n",
    "            if push_tag == u'推':\n",
    "                p += 1\n",
    "            elif push_tag == u'噓':\n",
    "                b += 1\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "        # 統計推噓文\n",
    "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "        # all 為總共留言數量 \n",
    "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "        # 整理文章資訊\n",
    "        data = {\n",
    "            'url': self.url,\n",
    "            'article_author': author,\n",
    "            'article_title': title,\n",
    "            'article_date': date,\n",
    "            'article_content': content,\n",
    "            'ip': ip,\n",
    "            'message_count': message_count,\n",
    "            'messages': messages\n",
    "        }\n",
    "        return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    all_url = []\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                all_url.append(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(len(all_url)))\n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(len(all_url)):\n",
    "        threads.append(Crawl_Article(all_url[i]))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876536.A.B2E.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876557.A.92D.html\n",
      "Parse Re: [問卦] 開蟑聖母現在在想什麼？ - https://www.ptt.cc/bbs/Gossiping/M.1582876570.A.5F5.html\n",
      "Parse Re: [爆卦] 第三十三案、第三十四案 LIVE記者會 - https://www.ptt.cc/bbs/Gossiping/M.1582876582.A.45D.html\n",
      "Parse [問卦] 欸！究竟怎麼防中國網軍啊？ - https://www.ptt.cc/bbs/Gossiping/M.1582876639.A.08A.html\n",
      "Parse Re: [新聞] 武漢肺炎》居家檢疫失聯 新竹縣府協尋「 - https://www.ptt.cc/bbs/Gossiping/M.1582876679.A.EBA.html\n",
      "Parse [問卦] 這次日本會不會蓋牌得太誇張 - https://www.ptt.cc/bbs/Gossiping/M.1582876682.A.C63.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876702.A.579.html\n",
      "Reach the last article\n",
      "共8個連結\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876536.A.B2E.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876557.A.92D.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876570.A.5F5.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876582.A.45D.html\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876639.A.08A.html\n",
      "\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876679.A.EBA.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876682.A.C63.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1582876702.A.579.html\n",
      "\n",
      "共用時： 0.5119025707244873\n"
     ]
    }
   ],
   "source": [
    "# 使用佇列 Queue\n",
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子執行緒 100 取得lock\n",
      "子執行緒 100: 寫入檔案 Url 1\n",
      "子執行緒 100 釋放lock\n",
      "子執行緒 200 取得lock\n",
      "子執行緒 200: 寫入檔案 Url 2\n",
      "子執行緒 200 釋放lock\n",
      "子執行緒 200 取得lock\n",
      "子執行緒 200: 寫入檔案 Url 4\n",
      "子執行緒 200 釋放lock\n",
      "子執行緒 100 取得lock\n",
      "子執行緒 100: 寫入檔案 Url 3\n",
      "子執行緒 100 釋放lock\n",
      "子執行緒 200 取得lock\n",
      "子執行緒 200: 寫入檔案 Url 5\n",
      "子執行緒 200 釋放lock\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 使用lock:\n",
    "# 被 Lock 的 acquire 與 release 包起來的這段程式碼不會被兩個執行緒同時執行。 用來寫入檔案\n",
    "class Worker(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, num, lock):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.num = num\n",
    "        self.lock = lock\n",
    "\n",
    "    def run(self):\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "\n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            print(\"子執行緒 %d 取得lock\" % self.num)\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作\n",
    "            print(\"子執行緒 %d: 寫入檔案 %s\" % (self.num, url))\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 釋放 lock\n",
    "            print(\"子執行緒 %d 釋放lock\" % self.num)\n",
    "            self.lock.release()\n",
    "            \n",
    "#建立一個佇列\n",
    "my_queue = Queue()\n",
    "\n",
    "#假裝放五個URL進去queue\n",
    "for i in range(1,6):\n",
    "    my_queue.put(\"Url %d\" % i)\n",
    "\n",
    "# 建立 lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "#建立2個子執行緒，傳入queue和一個參數和lock\n",
    "my_worker1 = Worker(my_queue, 100, lock)\n",
    "my_worker2 = Worker(my_queue, 200, lock)\n",
    "\n",
    "my_worker1.start()\n",
    "my_worker2.start()\n",
    "\n",
    "my_worker1.join()\n",
    "my_worker2.join()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Re: [新聞] 富二代率眾毆死保全 公訴不受理 - https://www.ptt.cc/bbs/Gossiping/M.1582876536.A.B2E.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876557.A.92D.html\n",
      "Parse Re: [問卦] 開蟑聖母現在在想什麼？ - https://www.ptt.cc/bbs/Gossiping/M.1582876570.A.5F5.html\n",
      "Parse Re: [爆卦] 第三十三案、第三十四案 LIVE記者會 - https://www.ptt.cc/bbs/Gossiping/M.1582876582.A.45D.html\n",
      "Parse [問卦] 欸！究竟怎麼防中國網軍啊？ - https://www.ptt.cc/bbs/Gossiping/M.1582876639.A.08A.html\n",
      "Parse Re: [新聞] 武漢肺炎》居家檢疫失聯 新竹縣府協尋「 - https://www.ptt.cc/bbs/Gossiping/M.1582876679.A.EBA.html\n",
      "Parse [問卦] 這次日本會不會蓋牌得太誇張 - https://www.ptt.cc/bbs/Gossiping/M.1582876682.A.C63.html\n",
      "Parse Re: [新聞]拍電影燒光3千萬 阿Ken亮出6字底牌 - https://www.ptt.cc/bbs/Gossiping/M.1582876702.A.579.html\n",
      "Parse Re: [問卦] 人類有沒有可能只吃一種食物就能活 - https://www.ptt.cc/bbs/Gossiping/M.1582876714.A.0B6.html\n",
      "Reach the last article\n",
      "共9個連結\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-53:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\au\\Anaconda3\\envs\\spider\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-16-aecb56849ec5>\", line 150, in run\n",
      "    with open('../Data/PTT_Article.json', 'a+', encoding='utf-8') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../Data/PTT_Article.json'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 改寫：加入能寫入檔案的lock __有問題\n",
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, lock):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.lock = lock\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            #print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # 寫入檔案:單一文章內容\n",
    "            \n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            #print(\"%s 取得lock\" % url[32:51])\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作 : 將爬完的資訊存成 json 檔案\n",
    "            #print(\"寫入檔案\")\n",
    "            with open('../Data/PTT_Article.json', 'a+', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "                f.write(\",\")\n",
    "\n",
    "            # 釋放 lock\n",
    "            #print(\"%s 釋放lock\" % url[32:51])\n",
    "            self.lock.release()\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            \n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "                article_title = a_title.text\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "                article_title = a_title\n",
    "                \n",
    "            #article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 建立 lock\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url, lock))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myThread (threading.Thread):\n",
    "    def __init__(self, key_word_link, key_word, recursive):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.key_word_link = key_word_link\n",
    "        self.key_word = key_word\n",
    "        self.recursive = recursive\n",
    "    def run(self):\n",
    "        WikiArticle(self.key_word_link, self.key_word, self.recursive, multithread=True)\n",
    "\n",
    "def WikiArticle(key_word_link, key_word, recursive, multithread=False):\n",
    "    \n",
    "    if (recursive <= max_recursive_depth):\n",
    "        print(\"遞迴層[%d] - %s (%s)\" % (recursive, key_word_link, key_word))\n",
    "        \n",
    "        # 模擬封包的標頭\n",
    "        headers = {\n",
    "            'authority': 'zh.wikipedia.org',\n",
    "            'method': 'GET',\n",
    "            'path': '/wiki/' + key_word_link,\n",
    "            'scheme': 'https',\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "            'accept-encoding': 'gzip, deflate, br',\n",
    "            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',\n",
    "            'cookie': 'GeoIP=TW:TPE:Taipei:25.05:121.53:v4; TBLkisOn=0; mwPhp7Seed=8b8; WMF-Last-Access-Global=04-Jun-2019; WMF-Last-Access=04-Jun-2019',\n",
    "            'dnt': '1',\n",
    "            #'if-modified-since': 'Tue, 04 Jun 2019 12:03:22 GMT',\n",
    "            'referer': 'https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "        }    \n",
    "\n",
    "        url = 'https://zh.wikipedia.org' + key_word_link  # 組合關鍵字查詢URL\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.encoding = 'utf-8'\n",
    "\n",
    "        html = BeautifulSoup(resp.text, \"lxml\")\n",
    "        content = html.find(name='div', attrs={'id':'mw-content-text'}).find_all(name='p')\n",
    "        \n",
    "        #\n",
    "        # Part 1: 請參考範例1，爬取當前關鍵字的文章內容。\n",
    "        #         因為內容太多，我們把它寫入檔案，並以關鍵字作為檔案名稱，以便稍後查閱內容。\n",
    "        #         請先建立一個名為\"WikiArticle\"的資料夾，爬取到的文章內容會放在這個資料夾底下。\n",
    "        #\n",
    "        '''\n",
    "        \n",
    "        Your code here\n",
    "        \n",
    "        '''\n",
    "        dir_ = 'WikiArticle' #存放資料夾\n",
    "        if not os.path.exists(dir_):\n",
    "            os.makedirs(dir_)\n",
    "        \n",
    "        with open('{}/{}.txt'.format(dir_, key_word.replace('/', '_')), 'w', encoding='utf8') as f_:\n",
    "            for paragraph in content:\n",
    "                f_.write(paragraph.get_text())\n",
    "            \n",
    "\n",
    "        \n",
    "        #\n",
    "        # Part 2: 請參考範例2，萃取出本篇文章中所延伸引用的外部連結，並儲存在external_link_dict\n",
    "        #\n",
    "        external_link_dict = dict({})\n",
    "        '''\n",
    "        \n",
    "        Your code here\n",
    "        \n",
    "        '''\n",
    "        for ext_link in content:\n",
    "            a_tag = ext_link.find_all('a', href=re.compile(\"^(/wiki/)((?!;)\\S)*$\"))\n",
    "            if len(a_tag) > 0:\n",
    "                for link_string in a_tag:\n",
    "                    a_link = link_string[\"href\"]       # 外部連結的網址\n",
    "                    a_keyword = link_string.get_text()  # 外部連結的中文名稱\n",
    "                    external_link_dict[a_link] = a_keyword\n",
    "\n",
    "                    \n",
    "        #\n",
    "        # Part 3: 將Part 2所收集的外部連結，當作新的關鍵字，繼續迭代深入爬蟲\n",
    "        #\n",
    "        if (len(external_link_dict) > 0):\n",
    "            \n",
    "            recursive = recursive + 1  # 遞迴深度加1\n",
    "            \n",
    "            thread_list = []\n",
    "            for k, v in external_link_dict.items():\n",
    "                if multithread:\n",
    "                    thread = myThread(k, v, recursive)\n",
    "                    thread_list.append(thread)\n",
    "                    thread.start()\n",
    "                else:\n",
    "                    WikiArticle(k, v, recursive)  # 再次呼叫同樣的函數，執行同樣的流程\n",
    "                    \n",
    "            if multithread:\n",
    "                for thread in thread_list:\n",
    "                    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: RNN\n",
      "/wiki/RNN\n"
     ]
    }
   ],
   "source": [
    "input_keyword = \"RNN\"  # 這裡可以自己定義有興趣的關鍵字\n",
    "\n",
    "utf8_url = repr(input_keyword.encode('UTF-8')).upper()  # 編碼成UTF-8並轉成大寫字元\n",
    "utf8_url = utf8_url.replace(\"\\\\X\", \"%\")                 # 用 '%' 取代 '\\X' \n",
    "print(\"%s: %s\" % (input_keyword, utf8_url[2:-1:1]))     # 擷取中間的編碼結果\n",
    "\n",
    "# 組成Wiki關鍵字搜尋的網址格式\n",
    "root_keyword_link = '/wiki/' + utf8_url[2:-1:1]\n",
    "print(root_keyword_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遞迴層[0] - /wiki/RNN (RNN)\n",
      "遞迴層[1] - /wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98 (梯度消失問題)\n",
      "遞迴層[1] - /wiki/LSTM (LSTM)\n",
      "遞迴層[1] - /wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C (前饋神經網絡)\n",
      "遞迴層[1] - /wiki/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97 (時間序列)\n",
      "遞迴層[1] - /wiki/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB (手寫識別)\n",
      "遞迴層[1] - /wiki/Hopfield%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C (Hopfield神經網絡)\n",
      "遞迴層[1] - /wiki/%E7%99%BE%E5%BA%A6 (百度)\n",
      "遞迴層[1] - /wiki/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90 (文本到語音合成)\n",
      "遞迴層[1] - /wiki/Android (安卓系統)\n",
      "遞迴層[1] - /wiki/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91 (機器翻譯)\n",
      "遞迴層[1] - /wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B (語言建模)\n",
      "遞迴層[1] - /wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C (卷積神經網絡)\n",
      "遞迴層[1] - /wiki/%E5%9B%BE%E5%83%8F%E8%87%AA%E5%8A%A8%E6%A0%87%E6%B3%A8 (圖像自動標註)\n",
      "遞迴層[1] - /wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86 (自然語言處理)\n",
      "遞迴層[1] - /wiki/%E9%9D%9E%E7%BA%BF%E6%80%A7 (非線性方程)\n",
      "遞迴層[1] - /wiki/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0 (Logistic方程)\n",
      "遞迴層[1] - /wiki/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0 (監督學習)\n",
      "遞迴層[1] - /wiki/%E8%A7%A3%E7%A0%81%E5%99%A8 (解碼器)\n",
      "遞迴層[1] - /wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92 (監督學習)\n",
      "遞迴層[1] - /wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0 (強化學習)\n",
      "遞迴層[1] - /wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8 (多層感知器)\n",
      "41.64397954940796\n"
     ]
    }
   ],
   "source": [
    "# 單線程爬蟲\n",
    "# 定義爬取的遞迴深度。深度不要訂太深，否則會爬很久。\n",
    "max_recursive_depth = 1\n",
    "\n",
    "startTime = time.time()\n",
    "WikiArticle(root_keyword_link, input_keyword, 0)\n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime) # 正常情況的爬蟲所需時間\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遞迴層[0] - /wiki/RNN (RNN)\n",
      "遞迴層[1] - /wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98 (梯度消失問題)遞迴層[1] - /wiki/LSTM (LSTM)\n",
      "\n",
      "遞迴層[1] - /wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C (前饋神經網絡)\n",
      "遞迴層[1] - /wiki/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97 (時間序列)\n",
      "遞迴層[1] - /wiki/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB (手寫識別)\n",
      "遞迴層[1] - /wiki/Hopfield%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C (Hopfield神經網絡)\n",
      "遞迴層[1] - /wiki/%E7%99%BE%E5%BA%A6 (百度)\n",
      "遞迴層[1] - /wiki/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90 (文本到語音合成)\n",
      "遞迴層[1] - /wiki/Android (安卓系統)\n",
      "遞迴層[1] - /wiki/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91 (機器翻譯)\n",
      "遞迴層[1] - /wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B (語言建模)\n",
      "遞迴層[1] - /wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C (卷積神經網絡)\n",
      "遞迴層[1] - /wiki/%E5%9B%BE%E5%83%8F%E8%87%AA%E5%8A%A8%E6%A0%87%E6%B3%A8 (圖像自動標註)遞迴層[1] - /wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86 (自然語言處理)\n",
      "\n",
      "遞迴層[1] - /wiki/%E9%9D%9E%E7%BA%BF%E6%80%A7 (非線性方程)\n",
      "遞迴層[1] - /wiki/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0 (Logistic方程)\n",
      "遞迴層[1] - /wiki/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0 (監督學習)\n",
      "遞迴層[1] - /wiki/%E8%A7%A3%E7%A0%81%E5%99%A8 (解碼器)\n",
      "遞迴層[1] - /wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92 (監督學習)\n",
      "遞迴層[1] - /wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0 (強化學習)\n",
      "遞迴層[1] - /wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8 (多層感知器)\n",
      "6.4509687423706055\n"
     ]
    }
   ],
   "source": [
    "# 多線程爬蟲\n",
    "startTime = time.time()\n",
    "WikiArticle(root_keyword_link, input_keyword, 0, multithread=True)\n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime) # 利用 thread 的爬蟲所需時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打開瀏覽器\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_SO2(site):\n",
    "    browser = webdriver.Chrome(executable_path='chromedriver')\n",
    "\n",
    "    browser.get(\"http://taqm.epa.gov.tw/taqm/tw/MonthlyAverage.aspx\")\n",
    "\n",
    "    # 模擬使用者操作行為，選擇/點擊\n",
    "\n",
    "    selectSite = Select(browser.find_element_by_id(\"ctl05_ddlSite\"))\n",
    "    selectSite.select_by_value(site)\n",
    "    selectYear = Select(browser.find_element_by_id(\"ctl05_ddlYear\"))\n",
    "    selectYear.select_by_value('2018')\n",
    "\n",
    "    browser.find_element_by_id('ctl05_btnQuery').click()\n",
    "\n",
    "    locator=(\"id\", \"ctl05_gv\")\n",
    "    try:\n",
    "        WebDriverWait(browser, 30, 0.5).until(EC.presence_of_element_located(locator))\n",
    "    finally:\n",
    "        print('')\n",
    "\n",
    "    # 取得資料，丟到 BeautifulSoup 解析\n",
    "\n",
    "    html_source = browser.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    table = soup.find('table', class_='TABLE_G')\n",
    "    trs=table.find_all('tr')\n",
    "\n",
    "    bool_so2=False\n",
    "    str_date=''\n",
    "    str_value=''\n",
    "    for tr in trs:\n",
    "        tds=tr.find_all('td')\n",
    "        if (len(tds)>0):\n",
    "            if (len(tds)==5):\n",
    "                bool_so2=tds[0].text=='SO2'\n",
    "                if (bool_so2):\n",
    "                    str_date=tds[2].text\n",
    "                    str_value=tds[3].text\n",
    "            else:\n",
    "                str_date=tds[0].text\n",
    "                str_value=tds[1].text\n",
    "        if (bool_so2):\n",
    "            if (str_date<='2018/08'):\n",
    "                print(f\"{str_date} / {str_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018/01 / 1.80\n",
      "2018/02 / 1.90\n",
      "2018/03 / 2.20\n",
      "2018/04 / 2.30\n",
      "2018/05 / 3.10\n",
      "2018/06 / 2.70\n",
      "2018/07 / 2.20\n",
      "2018/08 / 2.40\n",
      "\n",
      "2018/01 / 2.40\n",
      "2018/02 / 2.60\n",
      "2018/03 / 2.90\n",
      "2018/04 / 3.20\n",
      "2018/05 / 4.20\n",
      "2018/06 / 3\n",
      "2018/07 / 3.10\n",
      "2018/08 / 3.90\n",
      "47.20784139633179\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "get_SO2('11')\n",
    "get_SO2('6')\n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime) # 正常情況的爬蟲所需時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "\n",
      "\n",
      "2018/01 / 1.80\n",
      "2018/01 / 2.402018/02 / 1.90\n",
      "2018/03 / 2.20\n",
      "2018/04 / 2.30\n",
      "2018/05 / 3.10\n",
      "2018/06 / 2.70\n",
      "2018/07 / 2.20\n",
      "2018/08 / 2.40\n",
      "\n",
      "2018/02 / 2.60\n",
      "2018/03 / 2.90\n",
      "2018/04 / 3.20\n",
      "2018/05 / 4.20\n",
      "2018/06 / 3\n",
      "2018/07 / 3.10\n",
      "2018/08 / 3.90\n"
     ]
    }
   ],
   "source": [
    "import _thread\n",
    "\n",
    "sites = ['11', '6']\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "for site in sites:\n",
    "    _thread.start_new_thread( get_SO2, (site,) )\n",
    "\n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime) # 利用 thread 的爬蟲所需時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "spider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
